# -*- coding: utf-8 -*-
"""Copy of Policy AI Agent RAG Prototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3KjaqMgfV6ph8FHwqOAPCEDBqHZbC_m
"""

!pip install -q langchain faiss-cpu sentence-transformers PyMuPDF

chat_history = []

topic_emojis = {
    "paternity": "üßë‚Äçüçº **Paternity Leave Policy**",
    "maternity": "ü§∞ **Maternity Leave Policy**",
    "leave": "üèñÔ∏è **Leave Policy**",
    "payroll": "üíµ **Payroll Information**",
    "insurance": "üõ°Ô∏è **Insurance Policy**",
    "referral": "ü§ù **Referral Bonus**",
    "dress": "üëî **Dress Code**",
    "default": "üìÑ **HR Policy Answer**"
}

!pip install -U langchain langchain-community faiss-cpu sentence-transformers pymupdf

from langchain_community.document_loaders import PyMuPDFLoader

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS

import os

from google.colab import files
uploaded = files.upload()

loader = PyMuPDFLoader("MPC - Policy Book - V1.5.pdf")
documents = loader.load()

print(f"Total pages loaded: {len(documents)}")
print("Sample text:\n", documents[0].page_content[:500])

text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
)

chunks = text_splitter.split_documents(documents)
print(f"Total chunks created: {len(chunks)}")
print("Sample chunk:\n", chunks[10].page_content)

# Load embedding model
embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

db = FAISS.from_documents(chunks, embedding_model)

db.save_local("mpc_faiss_index")

query = "What is the paternity leave policy?"
docs = db.similarity_search(query, k=2)

for i, doc in enumerate(docs):
    print(f"\nMatch {i+1}:\n{doc.page_content}")

!pip install transformers

from transformers import pipeline

# Load the model
qa_model = pipeline("text2text-generation", model="google/flan-t5-base")

query = "What is the leave system?"
docs = db.similarity_search(query, k=2)

# Combine top chunks into one context string
context = "\n".join([doc.page_content for doc in docs])

prompt = f"""Answer the following question in clean, readable bullet points with each bullet point being in new line .

Context:
{context}

Question: {query}
"""

response = qa_model(prompt, max_length=256, do_sample=False)
print("Answer:", response[0]['generated_text'])

"""MISTRAL

"""

!pip install -U langchain langchain-community

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    openai_api_key="TOGETHERAI_API_KEY",  # Replace with your Together.ai API key
    model_name="mistralai/Mistral-7B-Instruct-v0.2",
    base_url="https://api.together.xyz/v1",  # Together.ai's endpoint
    temperature=0.2,
    max_tokens=512,
)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True
)

query = "what are the office timings?"
result = qa_chain.invoke(query)

print("üß† Answer:\n", result['result'])

# Optional: Show where the answer came from
#print("\nüîç Sources:")
#for doc in result['source_documents']:
#    print("-" * 40)
#    print(doc.page_content[:300])

"""Adding Memory Buffer"""

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_core.prompts import PromptTemplate

!pip install -U langchain langchain-community openai

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    openai_api_key="TOGETHERAI_API_KEY",  # Replace with your Together.ai API key
    model_name="mistralai/Mistral-7B-Instruct-v0.2",
    base_url="https://api.together.xyz/v1",  # Together.ai's endpoint
    temperature=0.2,
    max_tokens=512,
)

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(),
    memory=memory,
    return_source_documents=True
)

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ‚úÖ Define memory and tell it which output to store
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"  # üí• This is the missing piece!
)

# ‚úÖ Define the final chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(),
    memory=memory,
    return_source_documents=True
)

# ‚úÖ Ask a question (example)
query = "What are the working hours?"
result = qa_chain.invoke({"question": query})

# ‚úÖ Print the result
print("üß† HR Chatbot Answer:\n", result['answer'])

result = qa_chain.invoke({"question": "Is Saturday included?"})
print("üß† HR Chatbot Answer:\n", result['answer'])

import ipywidgets as widgets
from IPython.display import display, clear_output

# Widgets
chat_input = widgets.Text(
    placeholder='Ask something from the policy...',
    description='Query:',
    layout=widgets.Layout(width='80%')
)
send_button = widgets.Button(description="Send")
chat_output = widgets.Output()

# Function to handle chat logic
def handle_chat(_):
    query = chat_input.value.strip()
    if query == "":
        return

    chat_input.value = ""  # Clear input box

    with chat_output:
        print(f"üë§ You: {query}")
        try:
            result = qa_chain.invoke({"question": query})
            print("ü§ñ HR Bot:\n", result['answer'])
        except Exception as e:
            print("‚ö†Ô∏è Error:", e)

# Trigger chat on button click and Enter key
send_button.on_click(handle_chat)
chat_input.on_submit(handle_chat)  # üîë Add this line

# Optional Reset Button
reset_button = widgets.Button(description="üîÑ Reset Chat")
def reset_chat(_):
    memory.clear()
    with chat_output:
        clear_output()
        print("üß† Memory cleared. Start a fresh chat!")

reset_button.on_click(reset_chat)

# UI layout
ui = widgets.VBox([
    chat_output,
    widgets.HBox([chat_input, send_button]),
    reset_button
])

display(ui)
print("üí¨ HR Chatbot ready!")

"""Manually Trained Question Answers"""

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer, util
import torch

manual_qa = {
    "How do I raise a technical support ticket?": "To raise a technical support ticket, please email IT at itsupport@mpccloudconsulting.com with a short description and any screenshots of the issue.",
    "Who do I contact if my biometric punch-in is not recorded?": "If your punch-in is missed, please reach out to your reporting manager and send an email to attendance@mpccloudconsulting.com.",
    "Where can I check my leave balance?": "You can check your leave balance on the internal HRMS portal. If you don't have access, drop a note to hrsupport@mpccloudconsulting.com.",
    "Is there a canteen or food arrangement at the office?": "Yes, we have a pantry with tea and coffee. For meals, employees usually step out or order online. No full canteen service is available currently.",
    "What should I do if I forget my ID card?": "If you forget your ID card, inform your team lead and security. A temporary entry slip can be issued for the day.",
    "Are there any team outings or events planned?": "Team outings are planned on a quarterly basis. Keep an eye on the internal Teams channels or WhatsApp Group for any announcements.",
    "How do I get my Form 16?": "Form 16s are emailed directly by the finance team at the end of each financial year. You can request a duplicate from finance@mpccloudconsulting.com.",
    "Can I work remotely permanently?": "Permanent WFH is not currently offered. However, hybrid flexibility can be discussed with your reporting manager.",
    "Who do I talk to about payroll discrepancies?": "For payroll-related queries, email payroll@mpccloudconsulting.com. They usually respond within 1‚Äì2 business days.",
    #"How do I apply for reimbursement?": "Reimbursement claims can be submitted via the internal expense tool along with scanned bills. For help, contact expenses@mpccloudconsulting.com.",
    "What is the probation period?": "The standard probation period is 3 months, but it may vary based on role or department. Please check with your HR.",
    "Are internships paid?": "No, internships are generally not paid. The stipend amount is communicated in your offer letter or onboarding email if any.",
    "What‚Äôs the procedure for resignation?": "Resignation must be submitted via email to your reporting manager and HR. A notice period of 30‚Äì60 days applies.",
    "Can I change my shift timing?": "Shift changes can be requested through your manager and are approved based on team requirements.",
    "How are public holidays decided?": "Public holidays are based on the company‚Äôs annual holiday calendar, usually aligned with regional guidelines.",
    "Is there any cab/shuttle facility for work hours?": "Yes, there is the shuttle provided by the building that transports employees from metro station to the office building. The shuttle offers services every 15/20 minutes.",
    "Can I bring a guest to the office?": "Visitors must be approved by your team lead and the Admin team in advance. ID verification is required.",
    "Where do I submit travel reimbursement bills?": "All travel bills must be uploaded to the Travel module of the HRMS within 7 days of the trip.",
    "What‚Äôs the policy for internal transfers?": "Internal transfers can be requested after 6 months of tenure, subject to approval from both departments.",
    "Are Saturdays off?": "No, the office has 5 day work week from Monday to Friday. Saturdays and Sundays are off"
}


faq_encoder = SentenceTransformer('all-MiniLM-L6-v2')


faq_questions = list(manual_qa.keys())
faq_embeddings = faq_encoder.encode(faq_questions, convert_to_tensor=True)

def get_manual_answer(user_query, threshold=0.75):
    query_embedding = faq_encoder.encode(user_query, convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(query_embedding, faq_embeddings)[0]

    top_result = torch.topk(cosine_scores, k=1)
    score = top_result.values.item()
    index = top_result.indices.item()

    if score >= threshold:
        matched_question = faq_questions[index]
        return manual_qa[matched_question]
    else:
        return None

import ipywidgets as widgets
from IPython.display import display, clear_output


chat_input = widgets.Text(
    placeholder='Ask something from the policy...',
    description='Query:',
    layout=widgets.Layout(width='80%')
)
send_button = widgets.Button(description="Send")
chat_output = widgets.Output()

def query_handler(user_query):

    manual_response = get_manual_answer(user_query)
    if manual_response:
        return f"{manual_response}"


    if 'qa_chain' in globals():
        try:
            result = qa_chain.invoke({"question": user_query})
            return result['answer']
        except Exception as e:
            return f"‚ö†Ô∏è AI Error: {str(e)}"
    else:
        return "‚ö†Ô∏è AI model (qa_chain) is not loaded."


def handle_chat(_):
    query = chat_input.value.strip()
    if query == "":
        return

    chat_input.value = ""

    with chat_output:
        print(f"üë§ You: {query}")
        try:
            result = query_handler(query)
            print("ü§ñ HR Bot:\n", result)
        except Exception as e:
            print("‚ö†Ô∏è Error:", e)



send_button.on_click(handle_chat)
chat_input.on_submit(handle_chat)


reset_button = widgets.Button(description="üîÑ Reset Chat")
def reset_chat(_):
    memory.clear()
    with chat_output:
        clear_output()
        print("üß† Memory cleared. Start a fresh chat!")

reset_button.on_click(reset_chat)


ui = widgets.VBox([
    chat_output,
    widgets.HBox([chat_input, send_button]),
    reset_button
])

display(ui)
print("üí¨ HR Chatbot ready!")